[toc]

# 1. BiLSTM-CRF

Step1： Word Embedding

- 方法流程：
  - 将一个含有n个词的句子记作： x = (x1,x2,….xn）
  - 利用预训练的embedding矩阵将每个字映射为低维稠密的向量
- 重点理解
  - 预训练词向量为何可以提高模型泛化能力？
  - 有的在词向量后加Dropout的作用是什么？效果如何？

Step2： BiLSTM提取文本特征

- 方法流程
  - 将一个句子各个字的Embedding序列作为双向LSTM各个时间步的输入
  - 将正反向输出的隐状态进行拼接，得到完整的隐状态序列
- 重点理解
  - 为什么NER选择的神经网络结构从RNN演变到LSTM，再到BiLSTM?
  - 如何从公式层面上理解RNN提取序列化数据特征的弊端？
  - 如何从公式层面上理解LSTM的强大之处？
  - 为什么LSTM要演变成最终的BiLSTM?

Step3:得到P矩阵

- 方法流程
  - 将完整的隐状态序列接入线性层，从n维映射到k维，其中k是标注集的标签数
  - 从而得到自动提取的句子特征，记作矩阵 $P = (p_1,p_2,...,p_n)$，注意该矩阵是非归一化矩阵
  - 其中 $p_i$ 表示该单词对应各个类别的分数
- 如图所示，双向BiLSTM输出矩阵1.5（B-Person)， 0.9（IPerson)， 0.1（B-Organization),0.08(I-Organization)
- 这些分数将是CRF层的输入

![image-20221218183726191](https://s2.loli.net/2022/12/18/CtcWQ82u6p9rko1.png)

Step4:CRF层的引入

- 引入原因
  - NER是一类特殊的任务，因为表征标签的可解释序列“语法” 强加了几个硬约束, 可能的约束有：
    - 判定B-label1 I-label2 I-label3…为错误
    - 判定“O I-label” 是错误的
    - 命名实体的开头应该是“B-” 而不是“I-” 。
  - 而CRF层能够学习到句子的前后依赖，从而加入一些约束来保证最终预测结果有效

Step5：最终结果的计算

- CRF考虑前后标记依赖约束，综合使用标记状态转换概率作为
- 评分：
- 上式意为对整个序列x，整个序列标注的打分等于各个位置的打分之和，打分为2部分：
  - 前者由BiLSTM输出的 $P_i$ 决定
  - 后者由CRF转移矩阵 A 决定，其中 $A_{y_{i-1},y_i}$ 表示从第 $y_{i-1}$ 个标签到第 $y_i$ 个标签的转移得分。

$$
\large\text{score(x,y)}=\sum\limits_{i=1}^n{P_{i,y_i}}+\sum\limits_{i=1}^{n+1}A_{y_{i-1},y_i}
$$

