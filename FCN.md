[toc]

# 语义分割

## 实现流程

- 训练：根据batch size大小，将数据集中的训练样本和标签读 入卷积神经网络。根据实际需要，应先对训练图片及标签进 行预处理，如裁剪、数据增强 等。这有利于深层网络的的训练，加速收敛过程，同时也避免过拟合问题并增强了模型的泛化能力
- 验证：训练一个epoch结束后， 将数据集中的验证样本和标签 读入卷积神经网络，并载入训练权重。根据编写好的语义分 割指标进行验证，得到当前训 练过程中的指标分数，保存对 应权重。常用一次训练一次验 证的方法更好的监督模型表现
- 测试：所有训练结束后，将数据 集中的测试样本和标签读入卷 积神经网络，并将保存的最好 权重值载入模型，进行测试。 测试结果分为两种，一种是根据常用指标分数衡量网络性能， 另一种是将网络的预测结果以图片的形式保存下来，直观感受分割的精确程度

## 常用指标

假设:共有 $k+1$ 个类，$p_{ij}$ 表示本属于类 $i$ 但被预测为类 $j$ 的像素数量。即，$p_{ii}$ 表示真正的数量，而 $p_{ij}$ 和 $p_{ji}$ 则分别被解释为假正或假负。

Pixel Accuracy(PA 像素精度):标记正确的像素占总像素的比例。
$$
PA = \frac{\sum^k\limits_{i=0} p_{ii}}{\sum^k\limits_{i=0}\sum^k\limits_{j=1}p_{ij}}
$$
Mean Pixel Accuracy(MPA 均像素精度):计算每个类内 被正确分类像素数的比例，再求所有类的平均
$$
MPA = \frac{1}{k+1}\sum\limits_{i=0}^k\frac{p_{ii}}{\sum^k\limits_{j=0}p_{ij}}
$$
Mean Intersection over Union( MIoU 均交并比):计算真实值和预测值的交集和并集
$$
I o U=\frac{A_{\text {pred }} \cap A_{\text {true }}}{A_{\text {pred }} \cup A_{\text {true }}} \quad M I o U=\frac{1}{k+1} \sum_{i=0}^k \frac{p_{i i}}{\sum_{j=0}^k p_{i j}+\sum_{j=0}^k p_{j i}-p_{i i}}
$$

## 研究成果

1. 将分类网络改编为全卷积神经网络，具体包括全连接层转化为卷积层以及通过反卷积进行上采 样
2. 使用迁移学习的方法进行微调
3. 使用跳跃结构使得语义信息可以和表征信息相结合，产生准确而精细的分割
4. FCN证明了==端到端==、==像素到像素==训练方式下的卷积神经网络超过了现有语义分割方向最先进的技术
5. FCN成为了 PASCAL VOC 最出色的分割方法，较2011和2012分割算法的MIoU提高了将近20%

# 摘要

主要成就：端到端、像素到像素训练方式下的卷积神经网络超过了现有语义分割方向最先进的 技术

核心思想：搭建了一个全卷积网络，输入任意尺寸的图像，经过有效推理和学习得到相同尺寸 的输出

主要方法：将当前分类网络改编成全卷积网络(AlexNet、VGGNet和GoogLeNet)并进行微调设 计了跳跃连接将全局信息和局部信息连接起来，相互补偿

实验结果：在PASCAL VOC、NYUDv2和SIFT Flow数据集上得到了state-of-the-art的结果

### End-to-End

在计算机视觉领域，端到端可以简单地理解为， ==输入是原始图像，输出是预测图像，中间的具体过程依赖于算法本身的学习能力==。通过网络内部结构，对 原始图像进行降维和特征提取，并在后续过程中将尺寸较小的特征图逐渐恢复成与原图尺寸相同的预测图。

==特征提取==的好坏将直接影响最后的预测结果，端到端网络的最主要特点就是根据设计好的算法自己学习特征，而不需要人为干预。

### 分割术语

pixel-wise(pixels-to-pixels)：像素级别每张图片都是由一个个pixel组成的，pixel是图像的基本单位

image-wise：图像级别 比如一张图片的标签是狗，即“狗”是对整个图片的标注

patch-wise：块级别 介于像素级别和图像级别之间，每个patch都是由好多个pixel组成的

patchwise training：是指对每一个感兴趣的像素，以它为中心取一个patch(小块)，然后输入网络，输出则为该像素的 标签

# 引言&相关工作

在以往的分割方法中，主要有两大类缺点:

1. 基于图像块的分割虽然常见，但是效率低，且往往需要前期或者后期处理(例如超像素、检测框、 局部预分类等)
2. 语义分割面临着语义和位置信息不可兼得的问题。全局信息解决的“是什么”，而局部信息解决的是“在哪里”

为了解决上面这两个问题，本文主要有三个创新点:

1. 将分类网络改编为全卷积神经网络，具体包括全连接层转化为卷积层以及通过反卷积进行上采样 2. 使用迁移学习的方法进行微调
2. 使用跳跃结构使得语义信息可以和表征信息相结合，产生准确而精细的分割

对应论文1.Introduction & 2.Related work

## 全局信息与局部信息

局部信息

-  提取位置:浅层网络中提取局部信息
- 特点:物体的几何信息比较丰富，对应的感受野较小
- 目的:有助于分割尺寸较小的目标，有利于提高分割的精确程度

全局信息

- 提取位置:深层网络中提取全局信息
- 特点:物体的空间信息比较丰富，对应的感受野较大
- 目的:有助于分割尺寸较大的目标，有利于提高分割的精确程度

## 感受域

在卷积神经网络中，决定某一层输出结果中一个 元素所对应的输入层的区域大小，被称作感受野。

通常来说，大感受野的效果要比小感受野的效果更好。由公式可见，==stride越大，感受野越大==。 但是过大的stride会使feature map保留的信息 变少。因此，在减小stride的情况下，如何增大感受野或使其保持不变，称为了分割中的一大问题。

对应原文3.Fully convolutional networks第一段结尾处

## 平移不变性

Translation invariance

宏观结果:图像中的目标无论被移到 图片中的哪个位置，分类结果都应该 是相同的

具体过程:卷积 & 最大池化 ≈ 平移 不变图像中的目标有移动时，得到的 特征图也会产生相同移动

<img src="https://s2.loli.net/2022/11/20/Psjx2QnITJZfA4W.png" alt="image-20221120165634834" style="zoom: 60%;" />

## 经典算法 VS 本文算法

Classical Algorithm VS FCN Algorithm

对应原文3.1 Adapting classifiers for dense prediction

![image-20221124231845010](https://s2.loli.net/2022/11/24/EvnaTGbpWJr8FUz.png)

FCN网络中，将CNN网络的后三层全部转化为$1\times1$ 的卷积核所对应等同向量长度的多通道卷积层。整个网络模型全部都由卷积层组成，没有全连接层产生的向量。CNN是图像级的识别，也就是从图像到结果。而FCN是像素级的识别，标注出输入图像上的每一个像素最可能属于哪一类别。

## Shift-and-stitch

<img src="https://s2.loli.net/2022/11/24/epQ7YoTE8GuPwDF.png" alt="image-20221124232100598" style="zoom:67%;" />

补零 + 平移原始图片得到四种版本的输入图片

- 最大池化得到对应的四张输出特征图
- 将四张输出图拼接成密集预测图

## 上采样

Upsampling

本文没有沿用以往的插值上采样 (Interpolation)，而是提出了新的上采样方法，即反卷积(Deconvolution)。

反卷积可以理解为卷积操作的逆运算，反卷积并不能复原因卷积操作造成的值的损失，它仅仅是将卷积过程中的步骤反向变换一次，因此它还可以被称为转置卷积。

下采样 $\rm Output = \frac{input-k+2p}{stride}+1$

上采样 $\rm Output = (input-1)\times stride+k-2p$

# 模型详解

## 算法架构

<img src="https://s2.loli.net/2022/11/24/hYMDyfzpUTF1oxl.png" alt="image-20221124232527755" style="zoom:45%;" />

<img src="https://s2.loli.net/2022/11/24/QusLm3q9F7HovxA.png" alt="image-20221124232604126" style="zoom:50%;" />

## 训练技巧

1. 加载预训练模型
2. 初始化反卷积参数
3. 至少175个epoch后算法才会有不错的表现
4. 学习率在100次后进行调整
5. pool3 之前的特征图不需要融合 

![image-20221124232748724](https://s2.loli.net/2022/11/24/a21o58JzkXBFu4m.png)

# 实验结果及分析

硬件设备:NVIDIA Tesla K40c

深度学习框架:Caffe minibatch:20

优化器:SGD + momentum 0.9

学习率:

- FCN-AlexNet 10-3
- FCN-VGG 16 10-4
- FCN-GoogLeNet 5-5

分类模型中的 Dropout

扩大数据规模 数据预处理:Randomly mirroring

类别平衡并不是必须的（不太正确）

# 讨论和总结

## 讨论

问题：类别平衡真的不是必须的吗? 以分类问题为例。假设一个训练 集共包含99张狗狗的图片和5张小 猫的图片，测试集包含99张狗狗 的图片和1张小猫的图片，你认为 算法最后的测试准确率是多少?

解决：一定要保证类别的平衡性，在 LinkNet论文中会讲解具体方法

## 总结

关键点&创新点

- 对经典网络的改编——卷积替换全连接
- 对前后特征图的补偿——跳跃连接
- 对特征图尺寸的恢复——反卷积

改进点

- 尺寸恢复
- 类别平衡
- 数据预处理
- 资源利用
