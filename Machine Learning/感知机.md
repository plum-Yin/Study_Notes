[toc]

# 1. 概述

超平面 $\mathbf{S}$ 是比特征空间 $\mathbf{R}^n$ 小 $1$ 维的子空间（ $n-1$ 维）

方法 = 模型 + 策略 + 算法

- 模型：感知机
- 策略：损失函数最小
- 方法：随机梯度下降法 SGD

---

# 2. 模型

定义 2.1

假设输入空间（特征空间）是 $\mathcal{X} \subseteq \mathbf{R}^{n}$, 输出空间是 $\mathcal{Y}=\{+1,-1\}$ 。输入 $x \in \mathcal{X}$ 表示实例的特征向量，对应于输入空间（特征空间）的点；输出 $y \in \mathcal{Y}$ 表示实例的类别。由输入空间到输出空间的如下函数:
$$
f(x)=\operatorname{sign}(w \cdot x+b)\quad\quad\text{(2.1)}
$$
称为感知机。其中 $w$ 和 $b$ 为感知机模型参数

- $ w\in \mathbf{R}^{n}$ 叫作权值（weight）或权值向量 (weight vector）
- $b\in \mathbf{R}$ 叫作偏置（bias）
- $w \cdot x$ 表示 $w$ 和 $x$ 的内积。
- sign 是符号函数, 即

$$
\operatorname{sign}(x)= \begin{cases}+1, & x \geqslant 0 \\ -1, & x<0\end{cases}\quad\quad\text{(2.2)}
$$
感知机是一种线性分类模型，属于判别模型。

感知机模型的假设空间是定义在特征空间中的所有线性分类模型（linear classification model）或线性分类器（linear classifier）, 即函数集合 $\{f \mid f(x)=w \cdot x+b\}$ 。

感知机有如下几何解释：线性方程
$$
w\cdot x+b=0\quad\quad\text{(2.3)}
$$
对应于特征空间 $\mathbf{R}^{n}$ 中的一个超平面 $S$，其中 $w$ 是超平面的法向量，$b$ 是超平面的截距。这个超平面将特征空间划分为两个部分。位于两部分的点（特征向量）分别被分为正、负两类。

因此，超平面 $S$ 称为分离超平面（separating hyperplane），如图2.1所示。

![image-20220513103138698](https://s2.loli.net/2022/05/13/16ELxqofHlhKGOZ.png)

感知机学习, 由训练数据集（实例的特征向量及类别）
$$
T=\left\{\left(x_{1}, y_{1}\right),\left(x_{2}, y_{2}\right), \cdots,\left(x_{N}, y_{N}\right)\right\}
$$
其中，$x_{i} \in \mathcal{X}=\mathbf{R}^{n}$，$y_{i} \in \mathcal{Y}=\{+1,-1\}, i=1,2, \cdots, N$，求得感知机模型 (2.1)，即求得模型参数 $w, b$ 。

感知机预测，通过学习得到的感知机模型，对于新的输入实例给出其对应的输出类别。

---

# 3. 学习策略

## 3.1 数据集的线性可分性

定义 2.2

给定一个数据集
$$
T=\left\{\left(x_{1}, y_{1}\right),\left(x_{2}, y_{2}\right), \cdots,\left(x_{N}, y_{N}\right)\right\}
$$
其中，$x_{i} \in \mathcal{X}=\mathbf{R}^{n}, y_{i} \in \mathcal{Y}=\{+1,-1\}, i=1,2, \cdots, N$, 如果存在某个超平面 $S$
$$
w \cdot x+b=0
$$
能够将数据集的正实例点和负实例点完全正确地划分到超平面的两侧，即对所有 $y_i=+1$ 的实例，有 $w·x_i+b>0$，对所有 $y_i=-1$ 的实例，有 $w\cdot x_i+b<0$，则称数据集 $T$ 为线性可分数据集（linearly separable data set）；否则，称数据集 $T$ 线性不可分。

---

## 3.2 学习策略

假设训练数据集是线性可分的，感知机学习的目标是求得一个能够将训练集正实例点和负实例点完全正确分开的分离超平面。为了找出这样的超平面，即确定感知机模型参数 $w$，$b$，需要确定一个学习策略，即定义（**经验）损失函数并将损失函数极小化**。

损失函数的一个自然选择是误分类点的总数。但是，这样的损失函数不是参数 $w$，$b$ 的连续可导函数，不易优化（这样的损失函数是离散的点：1,2,3,...）。损失函数的另一个选择是**误分类点到超平面 $S$ 的总距离**，这是感知机所采用的。

为此，首先写出输入空间 $\mathbf{R}^{n}$ 中任一点 $x_0$ 到超平面 $S$ 的距离：
$$
\frac{1}{\|w\|}\left|w \cdot x_{0}+b\right|
$$
这里, $\|w\|$ 是 $w$ 的 $L_{2}$ 范数。

其次, 对于误分类的数据 $\left(x_{i}, y_{i}\right)$ 来说，
$$
-y_{i}\left(w \cdot x_{i}+b\right)>0
$$
成立。因为当 $w \cdot x_{i}+b>0$ 时, $y_{i}=-1$; 而当 $w \cdot x_{i}+b<0$ 时, $y_{i}=+1$ 。

因此, **误分类点** $x_{i}$ 到超平面 $S$ 的距离是
$$
-\frac{1}{\|w\|} y_{i}\left(w \cdot x_{i}+b\right)
$$
这样，假设超平面 $S$ 的误分类点集合为 $M$, 那么所有误分类点到超平面 $S$ 的总 距离为
$$
-\frac{1}{\|w\|} \sum_{x_{i} \in M} y_{i}\left(w \cdot x_{i}+b\right)
$$
不考虑 $\frac{1}{\|w\|}$, 就得到感知机学习的损失函数。

---

感知机 $\operatorname{sign}(w \cdot x+b)$ 学习的损失函数定义为
$$
L(w,b)=-\sum\limits_{x_i\in M}y_i(w\cdot x_i+b)\quad\quad\text{(2.4)}
$$
其中 $M$ 为误分类点的集合。此损失函数就是感知机学习的经验风险函数。

---

# 4. 学习算法

感知机学习问题转化为求解损失函数式 (2.4) 的最优化问题, 最优化的方法是随 机梯度下降法。本节叙述感知机学习的具体算法, 包括原始形式和对偶形式, 并证明 在训练数据线性可分条件下感知机学习算法的收敛性。

## 4.1 原始形式

感知机学习算法是对以下最优化问题的算法。

给定一个训练数据集
$$
T=\left\{\left(x_{1}, y_{1}\right),\left(x_{2}, y_{2}\right), \cdots,\left(x_{N}, y_{N}\right)\right\}
$$
其中, $x_{i} \in \mathcal{X}=\mathbf{R}^{n}, y_{i} \in \mathcal{Y}=\{-1,1\}, i=1,2, \cdots, N$, 求参数 $w, b$, 使其为以下**损失函数极小化**问题的解
$$
\min _{w, b} L(w, b)=-\sum_{x_{i} \in M} y_{i}\left(w \cdot x_{i}+b\right)
\quad\quad\text{(2.5)}
$$
其中 $M$ 为误分类点的集合。

感知机学习算法是误分类驱动的，具体采用随机梯度下降法 (stochastic gradient descent）。

- 首先，任意选取一个超平面 $w_0$，$b_0$，然后用梯度下降法不断地极小化目标函数（2.5）。
- 极小化过程中不是一次使 $M$ 中所有误分类点的梯度下降，而是一次**随机**选取一个误分类点使其梯度下降。

假设误分类点集合 $M$ 是固定的，那么损失函数 $L(w,b)$ 的梯度由
$$
\nabla_{w} L(w, b)=-\sum_{x_{i} \in M} y_{i} x_{i}\\
\nabla_{b} L(w, b)=-\sum_{x_{i} \in M} y_{i}
$$
给出。

随机选取一个误分类点 $\left(x_{i}, y_{i}\right)$, 对 $w, b$ 进行更新:
$$
\begin{gathered}
w \leftarrow w+\eta y_{i} x_{i} \\
b \leftarrow b+\eta y_{i}
\end{gathered}
$$
式中 $\eta(0<\eta \leqslant 1)$ 是步长, 在统计学习中又称为学习率 (learning rate)。这样, 通过 迭代可以期待损失函数 $L(w, b)$ 不断减小，直到为 0 。

---

综上所述, 得到如下算法:

### 计算步骤(算法 2.1)

**感知机学习算法的原始形式**

- 输入: 训练数据集 $T=\left\{\left(x_{1}, y_{1}\right),\left(x_{2}, y_{2}\right), \cdots,\left(x_{N}, y_{N}\right)\right\}$, 其中
  - $x_{i} \in \mathcal{X}=\mathbf{R}^{n}, y_{i} \in \mathcal{Y}=\{-1,+1\}, i=1,2, \cdots, N$; 学习率 $\eta\;(0<\eta \leqslant 1)$;
- 输出: $w, b$；感知机模型 $f(x)=\operatorname{sign}(w \cdot x+b)$ 。

(1) 选取初值 $w_{0}, b_{0}$

(2) 在训练集中选取数据 $\left(x_{i}, y_{i}\right)$

(3) 如果 $y_{i}\left(w \cdot x_{i}+b\right) \leqslant 0$，即为误分类点
$$
\begin{aligned}
&w \leftarrow w+\eta y_{i} x_{i} \\
&b \leftarrow b+\eta y_{i}
\end{aligned}
$$
（4）转至（2），直至训练集中没有误分类点。

这种学习算法直观上有如下解释：当一个实例点被误分类，即位于分离超平面的错误一侧时，则调整 $w$，$b$ 的值，使分离超平面向该误分类点的一侧移动，以减少该误分类点与超平面间的距离，直至超平面越过该误分类点使其被正确分类。

算法 2.1 是感知机学习的基本算法，对应于后面的对偶形式，称为原始形式。感知机学习算法简单且易于实现。

---

## 4.2 收敛性

对于线性可分性数据集，感知机学习算法原始形式收敛，即经过有限次迭代可以得到一个将训练数据集完全正确划分的超平面及感知机模型。

**定理 2.1**

(Novikoff) 设训练数据集 $T=\left\{\left(x_{1}, y_{1}\right),\left(x_{2}, y_{2}\right), \cdots,\left(x_{N}, y_{N}\right)\right\}$ 是线 性可分的，其中

- $x_{i} \in \mathcal{X}=\mathbf{R}^{n}, y_{i} \in \mathcal{Y}=\{-1,+1\}, i=1,2, \cdots, N$, 则

( 1 ) 存在满足条件 $\left\|\hat{w}_{\mathrm{opt}}\right\|=1$ 的超平面 $\hat{w}_{\mathrm{opt}} \cdot \hat{x}=w_{\mathrm{opt}} \cdot x+b_{\mathrm{opt}}=0$ 将训练数 据集完全正确分开; 且存在 $\gamma>0$, 对所有 $i=1,2, \cdots, N$
$$
y_{i}\left(\hat{w}_{\mathrm{opt}} \cdot \hat{x}_{i}\right)=y_{i}\left(w_{\mathrm{opt}} \cdot x_{i}+b_{\mathrm{opt}}\right) \geqslant \gamma
$$
( 2 ) 令 $R=\max _{1 \leqslant i \leqslant N}\left\|\hat{x}_{i}\right\|$, 则感知机算法 2.1 在训练数据集上的误分类次数 $k$ 满足不等式
$$
k \leqslant\left(\frac{R}{\gamma}\right)^{2}
$$
定理表明，误分类的次数 $k$ 是有**上界**的，经过有限次搜索可以找到将训练数据完全正确分开的分离超平面。也就是说，当训练数据集线性可分时，感知机学习算法原始形式迭代是收敛的。

但是例 2.1 说明，感知机学习算法存在许多解，这些解既<u>依赖于初值的选择</u>，也<u>依赖于迭代过程中误分类点的选择顺序</u>。为了得到唯一的超平面，需要对分离超平面增加约束条件。这就是第 7 章将要讲述的线性支持向量机的想法。

当训练集线性不可分时，感知机学习算法不收敛，迭代结果会发生震荡。

## 4.3 对偶形式

对偶形式的基本想法是, 将 $w$ 和 $b$ 表示为实例 $x_{i}$ 和标记 $y_{i}$ 的线性组合的形式，通过求解其系数而求得 $w$ 和 $b$ 。

不失一般性，在算法 $2.1$ 中可假设初始值 $w_{0}, b_{0}$ 均为 0。 对误分类点 $\left(x_{i}, y_{i}\right)$ 通过
$$
\begin{aligned}
&w \leftarrow w+\eta y_{i} x_{i} \\
&b \leftarrow b+\eta y_{i}
\end{aligned}
$$
逐步修改 $w, b$, 设修改 $n$ 次, 则 $w, b$ 关于 $\left(x_{i}, y_{i}\right)$ 的增量分别是 $\alpha_{i} y_{i} x_{i}$ 和 $\alpha_{i} y_{i}$, 这里 $\alpha_{i}=n_{i} \eta$

这样, 从学习过程不难看出, 最后学习到的 $w, b$ 可以分别表示为
$$
\begin{gathered}
w=\sum_{i=1}^{N} \alpha_{i} y_{i} x_{i} \\
b=\sum_{i=1}^{N} \alpha_{i} y_{i}
\end{gathered}
$$
这里, $\alpha_{i} \geqslant 0, i=1,2, \cdots, N$, 当 $\eta=1$ 时, 表示第 $i$ 个实例点由于误分而进行更新的次数。

**实例点更新次数越多, 意昩着它距离分离超平面越近, 也就越难正确分类。换句话说, 这样的实例对学习结果影响最大。**

---

下面对照原始形式来叙述感知机学习算法的对偶形式。

算法 $2.2$ (感知机学习算法的对偶形式)

输入：线性可分的数据集 $T=\left\{\left(x_{1}, y_{1}\right),\left(x_{2}, y_{2}\right), \cdots,\left(x_{N}, y_{N}\right)\right\}$, 其中

- $x_{i} \in \mathbf{R}^{n}, y_{i} \in$ $\{-1,+1\}, i=1,2, \cdots, N$
- 学习率 $\eta\;(0<\eta \leqslant 1)$

输出：$\alpha, b$

- 感知机模型 $f(x)=\operatorname{sign}\left(\sum_{j=1}^{N} \alpha_{j} y_{j} x_{j} \cdot x+b\right)$, 其中 $\alpha=$ $\left(\alpha_{1}, \alpha_{2}, \cdots, \alpha_{N}\right)^{\mathrm{T}}$ 。

(1) $\alpha \leftarrow 0, b \leftarrow 0$;

(2) 在训栋集中选取数据 $\left(x_{i}, y_{i}\right)$;

(3) 如果 $y_{i}\left(\sum_{j=1}^{N} \alpha_{j} y_{j} x_{j} \cdot x_{i}+b\right) \leqslant 0$,
$$
\begin{aligned}
&\alpha_{i} \leftarrow \alpha_{i}+\eta \\
&b \leftarrow b+\eta y_{i}
\end{aligned}
$$
(4) 转至 (2) 直到没有误分类数据。

---

对偶形式中训练实例仅以内积的形式出现。为了方便, 可以预先将训练集中实例间的内积计算出来并以矩阵的形式存储，这个矩阵就是所谓的 Gram 矩阵（Gram matrix）
$$
\mathbf{G}=[x_i\cdot x_j]_{N\times N}
$$

---

