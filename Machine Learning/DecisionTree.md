[toc]

# 1. 概述

决策树（decision tree）是一种基本的分类与回归方法。决策树模型呈树状结构，在分类问题中，表示基于特征对实例进行分类的过程。他可以认为是if-then规则的集合，也可以认为是定义在特征空间与类空间上的条件概率分布。

学习时，利用训练数据，根据**损失函数最小化的原则建立决策树模型**。

预测时，对新的数据，利用决策树模型进行分类。

决策树学习通常包括3个步骤：**特征选择**、**决策树的生成**和**决策树修剪**。

决策树学习策略：以损失函数为目标函数的最小化

特征选择：递归地选择最优特征，并根据该特征对训练数据进行分隔，是得对各个子数据集有一个最好的分类决策树生成：与特征选择过程相对应，选择的特征依次形成决策树点的结点，直至所有训练数据子集都被基本正确分类，或无合适特征可选。

决策树的剪枝：为了避免过拟合，对已生成的树自下而上进行剪枝，将树变得更简单，从而使他具有更好的泛化能力。

---

# 2. 模型与学习

## 2.1 决策树模型

决策树定义：分类决策树模型是一种描述对实例进行分类的树形结构。决策树由结点（node）和有向边（directed edged）组成。结点有两种类型：内结点（internal node）和叶结点（leaf node）。内部结点表示一个特征或属性，叶结点表示一个类。

## 2.2 决策树与 if-then 规则

可以将决策树看成一个if-then规则的集合。将决策树转换成if-then规则的过程是这样的：由决策树的根结点到叶结点的每一条路径构建一条规则；路径上内部结点的特征对应着规则的条件，而叶结点的类对应着规则的结论。决策树的路径或其对应的if-then规则集合具有一个重要的性质：互斥并且完备。这就是说，每一个实例都被一条路径或一条规则所覆盖，而且只被一条路径或一条规则所覆盖。这里所谓覆盖是指实例的特征与路径上的特征一致或实例满足规则的条件。

**决策树得到的结果不是全局最优的，而是在每一个节点做的局部最优决策树得到的。**

<img src="https://s2.loli.net/2022/05/14/1j6rsH87bgA2B9o.png" alt="image-20220514143634569" style="zoom:67%;" />



## 2.3 决策树与条件概率分布

决策树还表示给定特征条件下类的条件概率分布。这一条件概率分布定义在特征空间的一个划分（partition）上。将特征空间划分为互不相交的单元（cell）或区域（region），并在每个单元定义一个类的概率分布就构成了一个条件概率分布。

**决策树的一条路径对应于划分中的一个单元**。决策树所表示的条件概率分布由各个单元给定条件下类的条件概率分布组成。假设 $X$ 为表示特征的随机变量，$Y$ 为表示类的随机变量，那么这个条件概率分布可以表示为 $P(Y|X)$。$X$ 取值于给定划分下单元的集合，$Y$ 取值于类的集合。各叶结点（单元）上的条件概率往往偏向某一个类，即**属于某一类的概率较大**。决策树分类时将该结点的实例强行分到条件概率大的那一类去。

<img src="https://s2.loli.net/2022/05/14/MgqsWQ5b8mL6PY1.png" alt="image-20220514144053856" style="zoom:67%;" />

图5.2（a）示意地表示了特征空间的一个划分。图中的大正方形表示特征空间。这个大正方形被若干个小矩形分割，每个小矩形表示一个单元。特征空间划分上的单元构成了一个集合，$X$ 取值为单元的集合。为简单起见，假设只有两类：正类和负类，即Y取值为 $+1$ 和 $-1$。小矩形中的数字表示单元的类。

图5.2（b）示意地表示特征空间划分确定时，**特征（单元）给定条件下类的条件概率分布**。图5.2（b）中条件概率分布对应于图5.2（a）的划分。当某个单元 $c$ 的条件概率满足 $P(Y=+1|X=c)>0.5$ 时，则认为这个单元属于正类，即落在这个单元的实例都被视为正例。

图5.2（c）为对应于图5.2（b）中条件概率分布的决策树。

---

## 2.4 决策树学习

假设给定训练数据集
$$
D=\{(x_1,y_1),(x_2,y_2),...,(x_N,y_N)\}
$$
其中，$x=({x_i}^{(1)},{x_i}^{(2)},...,{x_i}^{(n)})^T$ 为输入实例（特征向量），$n$ 为特征个数。$y_i=\{1,2,...,k\}$为类标记，$i=1,2,...,N$，$N$ 为样本容量。

决策树学习的目标是根据给定的训练数据集构建一个决策树模型，使他能够对实例进行正确的分类。\

决策树学习本质上是从训练数据集中归纳出一组分类规则。与训练数据集不相矛盾的决策树（即能对训练数据进行正确分类的决策树）可能有多个，也可能一个也没有。我们需要一个与训练数据矛盾较小的决策树，同时具有很好得泛化能力。从另一个角度看，决策树学习是由训练数据集估计条件概率模型。基于特征空间划分的类的条件概率有无穷个。我们选择的条件概率模型应该不仅对训练数据有很好的拟合，而且对未知数据有很好的预测。
决策树学习用损失函数表示这一目标。如下所述，决策树学习的损失函数通常是正则化的极大似然函数。决策树学习的策略是以损失函数为目标函数的最小化。
当损失函数决定以后，学习问题就变为在损失函数意义下选择最优决策树的问题。因为从所有可能的决策树中选取最优

决策树是NP complete 问题，所以现实中决策树学习算法通常采用启发式方式，近似求解这一最优化问题（**因此只能退而求其次地不断使用贪心算法划分区域得到决策规则**）。这样得到的决策树是次最优化（sub-optimal）的。

决策树学习的算法通常是一个递归第选择最优特征，并根据该特征对训练数据进行分割，使得对各个子数据集有一个最好的分类的过程。这一过程对应着对特征空间的划分，也对应着决策树的构建。开始，构建根结点，将所有训练数据都放在根结点。选择一个最优特征，按照这一特征将训练数据集分割成子集，使得各个子集有一个在当前条件下最好的分类。如果这些子集已经能够被**基本正确分类**，那么构建叶结点，并将这些子集分到所对应的叶结点中去；如果还有子集不能被基本正确分类，那么就对这些子集选择新的最优特征，继续对其进行分割，构建相应的结点。如此递归地进行下去，直到所有训练数据子集被基本正确分类，或者没有合适的特征为止。最后每个子集都被分到叶结点上，即都有了明确的类。这就是生成了一棵决策树。

以上方法生成的决策树可能对训练书记有很好的分类能力，但对未知的测试数据却未必有很好的分类能力，即可能发生过拟合现象。我们需要对己生成的树自下而上进行剪枝，将树变得更简单，从而使他具有更好的泛化能力。具体地，就是去掉过于细致的**叶结点**，使其退回到父结点，甚至更高的结点，然后将父结点或更高的结点改为新的叶结点。

如果特征数量很多，也可以在决策树学习开始的时候，对特征进行选择，只留下对训练数据有足够分类能力的特征。

可以看出，决策树学习算法包含**特征选择**、**决策树的生成**与**决策树的剪枝**过程。由于决策树表示一个条件概率分布，所以深浅不同的决策树对应着不同复杂度的概率模型。

- 决策树的生成对应于模型的局部选择，决策树的剪枝对应于模型的全局选择。
- 决策树只考虑局部最优（NP完全问题），
- 相应的，决策树的剪枝则考虑全局最优。

---

# 3. 特征选择

## 3.1 特征选择问题

特征选择在于选取对训练数据具有分类能力的特征。这样可以提高决策树学习的效率。

如果利用一个特征进行分类的结果与随机分类的结果没有很大差别，则称这个特征是没有分类能力的。经验上扔掉这样的特征对决策树学习的精度影响不大。通常特征选择的准则是信息增益或信息增益比。首先通过一个例子来说明特征选择问题。

<img src="https://s2.loli.net/2022/05/14/r4n9eBZfcbMSaQT.png" alt="image-20220514152529727" style="zoom: 80%;" />

希望通过所给的训练数据学习一个贷款申请的决策树，用以对未来的贷款申请进行分类，即当新的客户提出贷款申请时，根据申请人的特征利用决策树决定是否批准贷款申请。

特征选择是决定用哪个特征来划分特征空间。

希望通过所给的训练数据学习一个贷款申请的决策树，用以对未来的贷款申请进行分类，即当新的顾客提出贷款站时，根据申请人的特征利用决策树据的那个是否批准贷款申请。
特征选择时决定用哪个特征来划分特征空间。

下图表示从上表数据学习到的两个可能的决策树，分别由两个不同特征的根结点构成。（a）所示的根结点的特征是年龄，有3个取值，对应不同的取值有不同的子结点。两个决策树都可以从此延续下去。问题是：究竟选择哪一个特征更好些？这就要求确定选择特征的准则。

直观上，如果一个特征具有更好的分类能力，或者说，按照这一特征将训练数据集分隔成子集，使得各个子集在当前条件下有更好的分类，那么就更应该选择这个特征。信息增益（information gain）就能够很好地表示这一直观准则。

<img src="https://s2.loli.net/2022/05/14/gsuDNZwx1QTGa8C.png" alt="image-20220514152743534" style="zoom:80%;" />

---

## 3.2 信息增益

在信息论与概率统计中, 熵 (entropy) 是表示**随机变量不确定性的度量**。

设 $X$ 是 一个取有限个值的离散随机变量，其概率分布为
$$
P\left(X=x_{i}\right)=p_{i}, \quad i=1,2, \cdots, n
$$
随机变量 $x_i$ 的**信息量**为
$$
I(X=x_i)=\log\frac{1}{p(x_i)}
$$
则随机变量 $X$ 的熵定义为
$$
H(X)=-\sum_{i=1}^{n} p_{i} \log p_{i}
$$
在式 (5.1) 中, 若 $p_{i}=0$, 则定义 $0 \log 0=0$ 。通常, 式 (5.1) 中的对数以 2 为底或以 $\mathrm{e}$ 为底 (自然对数), 这时熵的单位分别称作比特 (bit) 或纳特 (nat)。由定义可知, 熵只依赖于 $X$ 的分布, 而与 $X$ 的取值无关, 所以也可将 $X$ 的熵记作 $H(p)$, 即
$$
H(p)=-\sum_{i=1}^{n} p_{i} \log p_{i}
$$
熵越大, 随机变量的不确定性就越大。从定义可验证
$$
0 \leqslant H(p) \leqslant \log n
$$

---

当随机变量只取两个值, 例如 1,0 时, 即 $X$ 的分布为
$$
P(X=1)=p, \quad P(X=0)=1-p, \quad 0 \leqslant p \leqslant 1
$$
熵为
$$
H(p)=-p \log _{2} p-(1-p) \log _{2}(1-p)
$$
这时, 熵 $H(p)$ 随概率 $p$ 变化的曲线如图 $5.4$ 所示 (单位为比特)。

<img src="https://s2.loli.net/2022/05/14/uPtHynw72AKLhTY.png" alt="image-20220514162614967" style="zoom:67%;" />

- 当 $p=0$ 或 $p=1$ 时 $H(p)=0$, 随机变量完全没有不确定性。
- 当 $p=0.5$ 时, $H(p)=1$, 熵取值最大, 随机变量不确定性最大。

设有随机变量 $(X, Y)$，其联合概率分布为
$$
P\left(X=x_{i}, Y=y_{j}\right)=p_{i j}, \quad i=1,2, \cdots, n ; \quad j=1,2, \cdots, m
$$
---

**条件熵** $H(Y \mid X)$ 表示在已知随机变量 $X$ 的条件下随机变量 $Y$ 的不确定性。

随机变量 $X$ 给定的条件下随机变量 $Y$ 的**条件熵** (conditional entropy) ，定义为 $X$ 给定条件下 $Y$ 的条件概率分布的樀对 $X$ 的数学期望
$$
H(Y \mid X)=\sum_{i=1}^{n} p_{i} H\left(Y \mid X=x_{i}\right)
$$
这里，$p_{i}=P\left(X=x_{i}\right), i=1,2, \cdots, n$ 。

当熵和条件熵中的概率由数据估计（特别是极大似然估计）得到时，所对应的熵与条件熵分别称为**经验熵**（empirical entropy）和**经验条件熵**（empirical conditional entropy）。

此时，如果有 $0$ 概率，令 $0 \log 0=0$ 。

---

**信息增益**（information gain）表示得知特征 $X$ 的信息而使得类 $Y$ 的信息的<u>不确定性减少的程度</u>。

定义5.2（信息增益）特征 $A$ 对训练数据集 $D$ 的信息增益 $g(D,A)$，定义为集合 $D$ 的经验嫡 $H(D)$ 与特征 $A$ 给定条件下 $D$ 的经验条件嫡 $H(D|A)$ 之差，即
$$
g(D,A)=H(D)-H(D|A)\quad\quad\text{(5.6)}
$$
一般地，熵 $H(Y)$ 与条件熵 $H(Y|X)$ 之差称为**互信息**（mutual information）。决策树学习中的信息增益等价于训练数据集中类与特征的互信息。

决策树学习应用信息增益准则选择特征。给定训练数据集 $D$ 和特征 $A$，经验熵 $H(D)$ 表示对数据集 $D$ 进行分类的不确定性。而经验条件熵 $H(D|A)$ 表示在特征 $A$ 给定的条件下对数据集 $D$ 进行分类的不确定性。那么它们的差，即信息增益，就表示由于特征 $A$ 而使得对数据集 $D$ 的分类的**不确定性减少的程度**。显然，对于数据集 $D$ 而言，信息增益依赖于特征，不同的特征往往具有不同的信息增益。信息增益大的特征具有更强的分类能力。

根据信息增益准则的特征选择方法是：对训练数据集（或子集）$D$，计算其每个特征的信息增益，并比较它们的大小，选择信息增益最大的特征。

- 设训练数据集为 $D$，$|D|$ 表示其样本容量，即样本个数。
- 设有 $K$ 个类 $C_k$，$k=1,2,...,K$，$|C_k|$ 为属于类 $C_k$ 的样本个数，$\sum\limits_{k=1}^K|C_k|=|D|$。
- 设特征 $A$ 有 $n$ 个不同的的取值 $\{a_1,a_2,...,a_n\}$，根据特征 $A$ 的取值将 $D$ 划分为 $n$ 个子集 $D_1,D_2,...,D_n$，$|D_i|$为 $D_i$ 的样本个数，$\sum\limits_{i=1}^n|D_i|=|D|$。记子集  $D_i$ 中属于类 $C_k$ 的样本的集合为 $D_{ik}$，即 $D_{ik}=D_i\cap C_k$，$|D_{ik}|$ 为 $D_{ik}$ 的样本个数。于是信息增益的算法如下。

---

### 3.2.1 信息增益算法

算法 5.1

输入: 训练数据集 $D$ 和特征 $A$;

输出: 特征 $A$ 对训练数据集 $D$ 的信息增益 $g(D, A)$ 。

（1）计算数据集 $D$ 的经验熵 $H(D)$
$$
H(D)=-\sum_{k=1}^{K} \frac{\left|C_{k}\right|}{|D|} \log _{2} \frac{\left|C_{k}\right|}{|D|}
$$
（2）计算特征 $A$ 对数据集 $D$ 的经验条件熵 $H(D \mid A)$
$$
H(D \mid A)=\sum_{i=1}^{n} \frac{\left|D_{i}\right|}{|D|} H\left(D_{i}\right)=-\sum_{i=1}^{n} \frac{\left|D_{i}\right|}{|D|} \sum_{k=1}^{K} \frac{\left|D_{i k}\right|}{\left|D_{i}\right|} \log _{2} \frac{\left|D_{i k}\right|}{\left|D_{i}\right|}
$$
（3）计算信息增益
$$
g(D, A)=H(D)-H(D \mid A)
$$

---

### 3.2.2 信息增益比

以信息增益作为划分训练数据集的特征, 存在偏向于选择取值较多的特征的问 题。使用信息增益比 (information gain ratio) 可以对这一问题进行校正。这是特征选 择的另一准则。

定义 $5.3$ (信息增益比) 特征 $A$ 对训练数据集 $D$ 的信息增益比 $g_{R}(D, A)$ 定义 为其信息增益 $g(D, A)$ 与训练数据集 $D$ 关于特征 $A$ 的值的熵 $H_{A}(D)$ 之比, 即
$$
g_{R}(D, A)=\frac{g(D, A)}{H_{A}(D)}
$$
其中, $H_{A}(D)=-\sum_{i=1}^{n} \frac{\left|D_{i}\right|}{|D|} \log _{2} \frac{\left|D_{i}\right|}{|D|}, n$ 是特征 $A$ 取值的个数。

---

# 4. 决策树的生成

##  4.1 ID3

ID3算法的核心是在决策树各个结点上应用信息增益准则选择特征，递归地构建决策树。

具体方法是：

- 从根结点（root node）开始，对结点计算所有可能的特征的信息增益，**选择信息增益最大的特征作为结点的特征**，由该特征的不同取值建立子结点；
- 再对子结点递归地调用以上方法，构建决策树；
- 直到所有特征的信息增益均很小或没有特征可以选择为止。
- 最后得到一棵决策树。

ID3相当于用极大似然法进行概率模型的选择。

---

算法5.2（ID3算法）

输入：训练数据集 $D$，特征集 $A$ 阀值 $\varepsilon$；

输出：决策树 $T$。

（1）若D中所有实例属于同一类 $C_k$，则 $T$ 为单结点树，并将类 $C_k$ 作为该结点的类标记，返回 $T$；

（2）若 $A=\empty$，则 $T$ 为单结点树，并将 $D$ 中实例数最大的类 $C_k$ 作为该结点的类标记，返回 $T$；

（3）否则，按算法5.1计算 $A$ 中各特征对 $D$ 的信息增益，选择信息增益最大的特征 $A_g$；

（4）如果 $A_g$ 的信息增益小于阀值 $\varepsilon$，则置 $T$ 为单结点树，并将 $D$ 中实例数最大的类 $C_k$ 作为该结点的类标记，返回 $T$；

（5）否则，对 $A_g$ 的每一可能值 $a_i$，依 $A_g=a_i$ 将 $D$ 分割为若干非空子集 $D_i$，将 $D_i$ 中实例数最大的类作为标记，构建子结点，由结点及其子结点构成树 $T$，返回 $T$；

（6）对第 $i$ 个子结点，以 $D_i$ 为训练集，以 $A-\{A_g\} $ 为特征集，递归地调用步（1）～步（5），得到子树 $T_i$，返回 $T_i$。

---

决策树lD3算法——小结

（1）ID3算法只有树的生成，所以该算法生成的树**容易产生过拟合**。

（2）ID3算法的基本思想是，以信息熵为度量，用于决策树结点的属性选择。

（3）每次优先选取信息量最多的属性，即能使熵值变为最小的属性，以构造一颗熵值下降最快的决策树，到叶子结点处的熵值为0.此时，每个叶子节点对应的实例集中的实例属于同一类。

---

## 4.2 C4.5

算法5.3（C4.5的生成算法）

输入：训练数据集 $D$，特征集 $A$ 阀值 $\varepsilon$；

输出：决策树 $T$。

（1）若D中所有实例属于同一类 $C_k$，则 $T$ 为单结点树，并将类 $C_k$ 作为该结点的类标记，返回 $T$；

（2）若 $A=\empty$，则 $T$ 为单结点树，并将 $D$ 中实例数最大的类 $C_k$ 作为该结点的类标记，返回 $T$；

（3）否则，按式（5.10）计算 $A$ 中各特征对 $D$ 的信息增益比，选择信息增益比最大的特征 $A_g$；

（4）如果 $A_g$ 的信息增益比小于阈值 $\varepsilon$，则置 $T$ 为单结点树，并将 $D$ 中实例数最大的类 $C_k$ 作为该结点的类，返回 $T$；

（5）否则，对 $A_g$ 的每一可能值 $a_i$，依 $A_g=a_i$ 将 $D$ 分割为若干非空子集 $D_i$，将 $D_i$ 中实例数最大的类作为标记，构建子结点，由结点及其子结点构成树 $T$，返回 $T$；

（6）对结点 $i$， 以 $D_i$ 为训练集，以 $A-\{A_g\}$ 为特征集，递归地调用步（1）~ 步（5），得到子树 $T_i$，返回 $T_i$。

---

# 5. 决策树的剪枝

决策树生成算法递归地产生决策树，直到不能继续下去为止。这样产生的树往往对训练数据的分类很准确，但对未知的测试数据的分类却没有那么准确，即出现==过拟合==现象。过拟合的原因在于学习时过多地考虑如何提高对训练数据的正确分类，从而构建出过于复杂的决策树。解决这个问题的办法是考虑决策树的复杂度，对已生成的决策树进行简化。

在决策树学习中将已生成的树进行简化的过程称为剪枝（pruning）。具体地，剪枝从已生成的树上裁掉一些子树或叶结点，并将其根结点或父结点作为新的叶结点，从而简化分类树模型。

---

主要有两种剪枝方法

- 前剪枝（pre-pruning）：这类方法用于决策树的生成过程中，通过一些阈值来限制决策树的生长，比如之前算法中的 e 等参数。这种方法在实际生产中用的多。
- 后剪枝（post-pruning）：这类方法则作用于决策树生成之后，主要的思想是对于一颗已生成的决策树，将其中不太必要的子树剪掉。（本书重点）

---

本节介绍一种简单的决策树学习的剪枝算法。

决策树的剪枝往往通过极小化决策树整体的损失函数（loss function）或代价函数（cost function）来实现。

设树 $T$ 的叶结点个数为 $|T|$，$t$ 是树 $T$ 的叶结点，该叶结点有 $N_{t}$ 个样本点, 

- 其中 $k$ 类的样本点有 $N_{t k}$ 个, $k=1,2, \cdots, K$
- $H_{t}(T)$ 为叶结点 $t$ 上的经验熵，$\alpha \geqslant 0$ 为参数，则决策树学习的**损失函数**可以定义为

$$
C_{\alpha}(T)=\sum_{t=1}^{|T|} N_{t} H_{t}(T)+\alpha|T|\quad\quad\text{(5.11)}
$$
其中经验熵为
$$
H_{t}(T)=-\sum_{k} \frac{N_{t k}}{N_{t}} \log \frac{N_{t k}}{N_{t}}\quad\quad\text{(5.12)}
$$
在损失函数中, 将式 (5.11) 右端的第 1 项记作
$$
C(T)=\sum_{t=1}^{|T|} N_{t} H_{t}(T)=-\sum_{t=1}^{|T|} \sum_{k=1}^{K} N_{t k} \log \frac{N_{t k}}{N_{t}}
\quad\quad\text{(5.13)}
$$
这时有
$$
C_{\alpha}(T)=C(T)+\alpha|T|
\quad\quad\text{(5.14)}
$$
式 (5.14) 中，

- $C(T)$ 表示模型对训练数据的预测误差，即模型与训练数据的拟合程度
- $|T|$ 表示模型复杂度
- 参数 $\alpha \geqslant 0$ 控制两者之间的影响。
  - 较大的 $\alpha$ 促使选择较简单的模型 (树)，较小的 $\alpha$ 促使选择较复杂的模型 (树)。 
  - $\alpha$ 大，损失函数对于 $|T|$ 越敏感
  - $\alpha=0$ 意味着只考虑模型与训练数据的拟合程度，不考虑模型的复杂度。

剪枝，就是当 $\alpha$ 确定时，选择损失函数最小的模型，即损失函数最小的子树。当 $\alpha$ 值确定时，子树越大，往往与训练数据的拟合越好，但是模型的复杂度就越高；相反，子树越小，模型的复杂度就越低，但是往往与训练数据的拟合不好。损失函数正好表示了对两者的平衡。

可以看出，决策树生成只考虑了通过提高信息增益（或信息增益比）对训练数据进行更好的拟合。而决策树剪枝通过优化损失函数还考虑了减小模型复杂度。决策树生成学习局部的模型，而决策树剪枝学习整体的模型。

式（5.11）或式（5.14）定义的损失函数的极小化等价于**正则化的极大似然估计**。所以，利用损失函数最小原则进行剪枝就是用正则化的极大似然估计进行模型选择。

图5.6是决策树剪枝过程的示意图。下面介绍剪枝算法。

---

算法5.4（树的剪枝算法）

输入：生成算法产生的整个树 $T$，参数 $\alpha$

输出：修剪后的子树 $T_\alpha$

（1）计算每个结点的经验熵。

（2）递归地从树的叶结点向上回缩。

<img src="https://s2.loli.net/2022/05/15/iPo5j9K2mSUXQ3L.png" alt="image-20220515161314142" style="zoom:67%;" />

设一组叶结点回缩到其父结点之前与之后的整体树分别为 $T_B$ 与 $T_A$，其对应的损失函数值分别是 $C_{\alpha}(T_B)$ 与 $C_{\alpha}(T_A)$，如果
$$
C_\alpha(T_A)\le C_\alpha(T_B)\quad\quad\text{(5.15)}
$$
则进行剪枝，即将父结点变为新的叶结点。

（3）返回（2），直至不能继续为止，得到损失函数最小的子树 $T_\alpha$

注意，式（5.15）只需考虑两个树的损失函数的差，其计算可以在局部进行。所以，决策树的剪枝算法可以由一种动态规划的算法实现。

---

# 6. CART 算法

CART 是在给定输入随机变量 $X$ 条件下输出随机变量Y的条件概率分布的学习方法。CART 假设决策树是二叉树，内部结点特征的取值为 “是” 和 “否”，左分支是取值为“是”的分支，右分支是取值为 “否” 的分支。这样的决策树等价于递归地二分每个特征，将输入空间即特征空间划分为有限个单元，并在这些单元上确定预测的概率分布，也就是在输入给定的条件下输出的条件概率分布。

CART算法由以下两步组成：

（1）决策树生成：基于训练数据集生成决策树，生成的决策树要尽量大；

（2）决策树剪枝：用验证数据集对已生成的树进行剪枝并选择最优子树，这时用损失函数最小作为剪枝的标准。

---

## 6.1 CART 生成

决策树的生成就是递归地构建二又决策树的过程。对回归树用平方误差最小化准则，对分类树用基尼指数（Gini index）最小化准则，进行特征选择，生成二叉树。

### 6.1.1 回归树的生成

假设 $X$ 与 $Y$ 分别为输入和输出变量, 并且 $Y$ 是连续变量，给定训练数据集 $D=\left\{\left(x_{1}, y_{1}\right),\left(x_{2}, y_{2}\right), \cdots,\left(x_{N}, y_{N}\right)\right\}$，考虑如何生成回归树。

一棵回归树对应着输入空间（即特征空间）的一个划分以及在划分的单元上的输出值。假设已将输入空间划分为 $M$ 个单元 $R_{1}, R_{2}, \cdots, R_{M}$，并且在每个单元 $R_{m}$ 上 有一个固定的输出值 ，于是回归树模型可表示为
$$
f(x)=\sum_{m=1}^{M} c_{m} I\left(x \in R_{m}\right)\quad\text{(5.16)}
$$
当输入空间的划分确定时，可以用平方误差 $\sum_{x_{i} \in R_{m}}\left(y_{i}-f\left(x_{i}\right)\right)^{2}$ 来表示回归树对 于训练数据的预测误差，用平方误差最小的准则求解每个单元上的最优输出值。易知，单元 $R_{m}$ 上的 $c_{m}$ 的最优值 $\hat{c}_{m}$ 是 $R_{m}$ 上的所有输入实例 $x_{i}$ 对应的输出 $y_{i}$ 的均值, 即
$$
\hat{c}_{m}=\operatorname{ave}\left(y_{i} \mid x_{i} \in R_{m}\right)
\quad\text{(5.17)}
$$
选择第 $j$ 个变量 $x^{(j)}$ 和它取的值 $s$，作为切分变量（splitting variable）和切分点（splitting point），并定义两个区域：

$$
R_1(j,s)=\{x|x^{(j)}\le s\}\quad\text{和}\quad R_2(j,s)=\{x|x^{(j)}> s\}
\quad\text{(5.18)}
$$


然后寻找最优切分变量 $j$ 和最优切分点 $s$ 。具体地, 求解
$$
\min _{j, s}\left[\min _{c_{1}} \sum_{x_{i} \in R_{1}(j, s)}\left(y_{i}-c_{1}\right)^{2}+\min _{c_{2}} \sum_{x_{i} \in R_{2}(j, s)}\left(y_{i}-c_{2}\right)^{2}\right]
\quad\text{(5.19)}
$$
对固定输入变量 $j$ 可以找到最优切分点 $s$ 。
$$
\hat{c}_{1}=\operatorname{ave}\left(y_{i} \mid x_{i} \in R_{1}(j, s)\right) \quad \text { 和 } \quad \hat{c}_{2}=\operatorname{ave}\left(y_{i} \mid x_{i} \in R_{2}(j, s)\right)
\quad\text{(5.20)}
$$
遍历所有输入变量，找到最优的切分变量 $j$，构成一个对 $(j, s)$ 。依此将输入空间划分为两个区域。接着, 对每个区域重复上述划分过程, 直到满足停止条件为止。

这样就生成一棵回归树。这样的回归树通常称为最小二乘回归树（least squares regression tree）, 现将算法叙述如下。

---

算法 $5.5$ (最小二乘回归树生成算法)

- 输入：训练数据集 $D$
- 输出：回归树 $f(x)$ 

在训练数据集所在的输入空间中, 递归地将每个区域划分为两个子区域并决定每 个子区域上的输出值，构建二叉决策树:

（1）选择最优切分变量 $j$ 与切分点 $s$, 求解
$$
\min _{j, s}\left[\min _{c_{1}} \sum_{x_{i} \in R_{1}(j, s)}\left(y_{i}-c_{1}\right)^{2}+\min _{c_{2}} \sum_{x_{i} \in R_{2}(j, s)}\left(y_{i}-c_{2}\right)^{2}\right]
\quad\text{(5.21)}
$$
遍历变量 $j$，对固定的切分变量 $j$ 扫描切分点 $s$，选择使式 (5.21) 达到最小值的对 $(j,s)$ 。

（2）用选定的对 $(j, s)$ 划分区域并决定相应的输出值:
$$
\begin{gathered}
R_{1}(j, s)=\left\{x \mid x^{(j)} \leqslant s\right\}\\
R_{2}(j, s)=\left\{x \mid x^{(j)}>s\right\} \\
\hat{c}_{m}=\frac{1}{N_{m}} \sum_{x_{i} \in R_{m}(j, s)} y_{i}, \quad x \in R_{m}, \quad m=1,2
\end{gathered}
$$
（3）继续对两个子区域调用步骤 (1), (2)，直至满足停止条件。

（4）将输入空间划分为 $M$ 个区域 $R_{1}, R_{2}, \cdots, R_{M}$, 生成决策树:
$$
f(x)=\sum_{m=1}^{M} \hat{c}_{m} I\left(x \in R_{m}\right)
$$

---

### 6.1.2 分类树的生成

分类树用基尼指数（Gini index）选择最优特征, 同时决定该特征的最优二值切分点。

定义 5.4 (基尼指数) 分类问题中，假设有 $K$ 个类，样本点属于第 $k$ 类的概率 为 $p_{k}$，则概率分布的基尼指数定义为
$$
\operatorname{Gini}(p)=\sum_{k=1}^{K} p_{k}\left(1-p_{k}\right)=1-\sum_{k=1}^{K} p_{k}^{2}
\quad\quad\text{(5.22)}
$$
对于 $2$ 类分类问题, 若样本点属于第 $1$ 个类的概率是 $p$，则概率分布的基尼指数为
$$
\operatorname{Gini}(p)=2 p(1-p)\quad\quad\text{(5.23)}
$$
对于给定的样本集合 $D$，其基尼指数为
$$
\operatorname{Gini}(D)=1-\sum_{k=1}^{K}\left(\frac{\left|C_{k}\right|}{|D|}\right)^{2}
\quad\quad\text{(5.24)}
$$
这里，$C_{k}$ 是 $D$ 中属于第 $k$ 类的样本子集，$K$ 是类的个数。

如果样本集合 $D$ 根据特征 $A$ 是否取某一可能值 $a$ 被分割成 $D_{1}$ 和 $D_{2}$ 两部分，即
$$
D_{1}=\{(x, y) \in D \mid A(x)=a\}, \quad D_{2}=D-D_{1}
$$
则在特征 $A$ 的条件下，集合 $D$ 的基尼指数定义为
$$
\operatorname{Gini}(D, A)=\frac{\left|D_{1}\right|}{|D|} \operatorname{Gini}\left(D_{1}\right)+\frac{\left|D_{2}\right|}{|D|} \operatorname{Gini}\left(D_{2}\right)
\quad\quad\text{(5.25)}
$$
基尼指数 $\operatorname{Gini}(D)$ 表示集合 $D$ 的**不确定性**，基尼指数 $\operatorname{Gini}(D, A)$ 表示经 $A=a$ 分割后集合 $D$ 的不确定性。基尼指数值越大，样本集合的不确定性也就越大，这一点与熵相似。

下图显示二类分类问题中基尼指数 $\operatorname{Gini}(p)$、熵（单位比特）之半 $H(p)/2$ 和分类误差率的关系。

横坐标表示概率 $p$，纵坐标表示损失。可以看出基尼指数和熵之半的曲线很接近，都可以近似地代表分类误差率。

<img src="https://s2.loli.net/2022/05/17/O8rIYx7UdbHoJh9.png" alt="image-20220517213206839" style="zoom:50%;" />

---

算法5.6（CART生成算法）

- 输入：训练数据集 $D$，停止计算的条件
- 输出：CART 决策树

根据训练数据集，从根结点开始，递归地对**每个结点**进行以下操作，构建二又决策树：

1. 设结点的训练数据集为 $D$，计算现有特征对该数据集的基尼指数。此时，对每一个特征 $A$，对其可能取的每个值 $a$，根据样本点对 $A=a$ 的测试为 “是” 或 “否” 将 $D$ 分割成 $D_1$ 和 $D_2$ 两部分，利用式（5.25）计算 $A=a$ 时的基尼指数。
2. 在所有可能的特征 $A$ 以及它们所有可能的切分点 $a$ 中，选择基尼指数最小的特征及其对应的切分点作为最优特征与最优切分点。依最优特征与最优切分点，从现结点生成两个子结点，将训练数据集依特征分配到两个子结点中去。
3. 对两个子结点递归地调用（1），（2），直至满足停止条件。
4. 生成 CART 决策树。

算法停止计算的条件是，

- 结点中的样本个数小于预定阀值，
- 或样本集的基尼指数小于预定阀值（样本基本属于同一类），
- 或者没有更多特征。

## 6.2 CART 剪枝

CART 剪枝算法从“完全生长” 的决策树的底端剪去一些子树，使决策树变小（模型变简单），从而能够对未知数据有更准确的预测。CART 剪枝算法由两步组成：

- 首先从生成算法产生的决策树 $T_0$ 底端开始不断剪枝，直到 $T_0$ 的根结点，形成一个子树序列 $\{T_0,T_1,...,T_n\}$；
- 然后通过交叉验证法在独立的验证数据集上对子树序列进行测试，从中选择最优子树。

---

<u>6.2.1 剪枝，形成一个子树序列</u>

在剪枝过程中，计算子树的损失函数：
$$
C_\alpha(T)=C(T)+\alpha|T|
\quad\quad\text{(5.26)}
$$
其中，$T$ 为任意子树，$C(T)$ 为对训练数据的预测误差（如基尼指数），$|T|$ 为子树的叶结点个数，$\alpha\ge0$ 为参数，$C_\alpha(T)$ 为参数是 $\alpha$ 时的子树 $T$ 的整体损失。参数 $\alpha$ 权衡训练数据的拟合程度与模型的复杂度。

对固定的 $\alpha$，一定存在使损失函数 $C_\alpha(T)$ 最小的子树，将其表示为 $T_\alpha$。

$T_\alpha$ 在损失函数 $C_\alpha(T)$ 最小的意义下是最优的。容易验证这样的最优子树是唯一的。

- 当 $\alpha$ 大的时候，最优子树 $T_\alpha$ 偏小；
- 当 $\alpha$ 小的时候，最优子树 $T_\alpha$ 偏大。
- 极端情况，当 $\alpha=0$ 时，整体树是最优的。
- 当 $\alpha\to\infty$ 时，根结点组成的单结点树是最优的。

Breiman 等人证明：可以用递归的方法对树进行剪枝。将 $\alpha$ 从小增大，$0=\alpha_0<\alpha_1<...<\alpha_n<+\infty$，产生一系列的区间 $[\alpha_i,\alpha_{i+1}),i=0,1,...,n$；剪枝得到的子树序列对应着区间 $\alpha \in\left[\alpha_{i}, \alpha_{i+1}\right), i=0,1, \cdots, n$ 的最优子树序列 $\left\{T_{0}, T_{1}, \cdots, T_{n}\right\}$, 序 列中的子树是嵌套的。

具体地, 从整体树 $T_{0}$ 开始前枝。对 $T_{0}$ 的任意内部结点 $t$，以 $t$ 为单结点（$|t|=1$）树的损失函数是
$$
C_{\alpha}(t)=C(t)+\alpha|t|=C(t)+\alpha
\quad\quad\text{(5.27)}
$$
以 $t$ 为根结点的子树 $T_{t}$ 的损失函数是
$$
C_{\alpha}\left(T_{t}\right)=C\left(T_{t}\right)+\alpha\left|T_{t}\right|
\quad\quad\text{(5.28)}
$$
当 $\alpha=0$ 及 $\alpha$ 充分小时，有不等式（对模型的复杂程度不敏感，完整的树损失最小）
$$
C_{\alpha}\left(T_{t}\right)<C_{\alpha}(t)
\quad\quad\text{(5.29)}
$$
当 $\alpha$ 增大时, 在某一 $\alpha$ 有
$$
C_{\alpha}\left(T_{t}\right)=C_{\alpha}(t)
\quad\quad\text{(5.30)}
$$
当 $\alpha$ 再增大时, 不等式 (5.29) 反向。只要 $\large\alpha=\frac{C(t)-C\left(T_{t}\right)}{\left|T_{t}\right|-1}$，$T_{t}$ 与 $t$ 有相同的损失函数值，而 $t$ 的结点少, 因此 $t$ 比 $T_{t}$ 更可取, 对 $T_{t}$ 进行剪枝。

为此, 对 $T_{0}$ 中每一内部结点 $t$, 计算
$$
g(t)=\frac{C(t)-C\left(T_{t}\right)}{\left|T_{t}\right|-1}
\quad\quad\text{(5.31)}
$$
它表示前枝后整体损失函数减少的程度。在 $T_{0}$ 中前去 $g(t)$ 最小的 $T_{t}$，将得到的子树 作为 $T_{1}$，同时将最小的 $g(t)$ 设为 $\alpha_{1}$ 。 $T_{1}$ 为区间 $\left[\alpha_{1}, \alpha_{2}\right)$ 的最优子树。

如此剪枝下去, 直至得到根结点。在这一过程中, 不断地增加 $\alpha$ 的值, 产生新的区间。

---

<u>6.2.2 在剪枝得到的子树序列 $T_0,T_1,...,T_n$ 中通过==交叉验证==选取最优子树 $T_\alpha$</u>

具体地，利用独立的验证数据集，测试子树序列 $T_0,T_1,...,T_n$ 中各棵子树的平方误差或基尼指数。

平方误差或基尼指数最小的决策树被认为是最优的决策树。

在子树序列中，每棵子树 $T_0,T_1,...,T_n$ 都对应于一个参数 $\alpha_0,\alpha_1,...,\alpha_n$ 。

所以，当最优子树 $T_k$ 确定时，对应的 $\alpha_k$ 也确定了，即得到最优决策树 $T_\alpha$

---

算法 5.7（CART 剪枝算法）

输入: CART 算法生成的决策树 $T_{0}$;

输出: 最优决策树 $T_{\alpha}$ 。

(1) 设 $k=0, T=T_{0}$ 。

(2) 设 $\alpha=+\infty$ 。

(3) 自下而上地对各内部结点 $t$ 计算 $C\left(T_{t}\right),\left|T_{t}\right|$ 以及
$$
\begin{aligned}
g(t) &=\frac{C(t)-C\left(T_{t}\right)}{\left|T_{t}\right|-1} \\
\alpha &=\min (\alpha, g(t))
\end{aligned}
$$
这里, $T_{t}$ 表示以 $t$ 为根结点的子树, $C\left(T_{t}\right)$ 是对训练数据的预测误差, $\left|T_{t}\right|$ 是 $T_{t}$ 的叶 结点个数。

（4）对 $g(t)=\alpha$ 的内部结点 $t$ 进行前枝, 并对叶结点 $t$ 以多数表决法决定其类, 得到树 $T$ 。

（5）设 $k=k+1, \alpha_{k}=\alpha, T_{k}=T$ 。

（6）如果 $T_{k}$ 不是由根结点及两个叶结点构成的树, 则回到步骤 (2); 否则令 $T_{k}=T_{n}$ 。

（7）采用交叉验证法在子树序列 $T_{0}, T_{1}, \cdots, T_{n}$ 中选取最优子树 $T_{\alpha}$ 。

